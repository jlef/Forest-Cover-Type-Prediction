if(res != start.Matrix[i,j+0]){
print(paste0('start.Matrix:',start.Matrix[i,j]+0,' pred:',res, ' gdata[5]:',gdata[5]));
}
result <- c(result, res)
}
MResultValid <- rbind(MResultValid, result)
}
require('klaR')
errormatrix(na.omit(as.vector(MResultValid+0)), na.omit(as.vector(start.Matrix[1,]+0)), relative = F)
errormatrix(na.omit(as.vector(MResultValid+0)), na.omit(as.vector(start.Matrix[1,]+0)), relative = T)
MResult[MResult==-1]<-0
MResultValid[MResultValid==-1]<-0
errormatrix(na.omit(as.vector(MResultValid+0)), na.omit(as.vector(start.Matrix[1,]+0)), relative = F)
errormatrix(na.omit(as.vector(MResultValid+0)), na.omit(as.vector(start.Matrix[1,]+0)), relative = T)
source('~/.active-rstudio-document', echo=TRUE)
errormatrix(na.omit(as.vector(MResultValid+0)), na.omit(as.vector(start.Matrix+0)), relative = F)
errormatrix(na.omit(as.vector(MResultValid+0)), na.omit(as.vector(start.Matrix+0)), relative = T)
save.image("C:/Users/GALT/Desktop/Game of Life/nnet/2.RData")
save.image("C:/Users/GALT/Desktop/Game of Life/nnet/2.RData")
load("C:/Users/GALT/Desktop/Game of Life/nnet feedback/completed.RData")
load("C:/Users/GALT/Desktop/Game of Life/nnet feedback/completed.RData")
load("C:/Users/GALT/Desktop/Game of Life/nnet feedback/completed.RData")
rm(totalSum_off)
rm(totalSum_on)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
sum(input)
input
?omit.na
?omitna
?na.omit
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('C:/Users/GALT/Desktop/Game of Life/nnet feedback/0.functions.R', echo=TRUE)
source('C:/Users/GALT/Desktop/Game of Life/nnet feedback/0.gameOfLife.R', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
load("C:/Users/GALT/Desktop/Allstate Purchase Prediction Challenge/code/.RData")
mytest$cbn <- paste0(mytest$A, mytest$B, mytest$C, mytest$D, mytest$E, mytest$F, mytest$G)
mydata$cbn <- paste0(mydata$A, mydata$B, mydata$C, mydata$D, mydata$E, mydata$F, mydata$G)
purchases <- mydata[ !duplicated( mydata$customer_ID, fromLast=TRUE ) , ]
quotesTop_test <- mytest[ mytest$record_type == 0, ]
quotesTop_test <- quotesTop_test[ !duplicated( quotesTop_test$customer_ID, fromLast=FALSE ), ]
quotesTop$C_previous[is.na( quotesTop$C_previous )] <- 0
purchases$C_previous[is.na( purchases$C_previous )] <- 0
quotesTop$PA <- purchase$A
quotesTop$PB <- purchase$B
quotesTop$PC <- purchase$C
quotesTop$PD <- purchase$D
quotesTop$PE <- purchase$E
quotesTop$PF <- purchase$F
quotesTop$PG <- purchase$G
library(rpart)
require(caret)
set.seed(3456)
inTrain <- createDataPartition(y=quotesTop$customer_ID, p=0.75, list=FALSE);   # We wish 75% for the trainset
train.set <- quotesTop[inTrain,];
valid.set  <- quotesTop[-inTrain,];
fit <- rpart(PC ~ married_couple + C_previous + C, data=valid.set, method="class")
fit <- rpart(PC ~ married_couple + C_previous + C, data=train.set, method="class")
Prediction <- predict(fit, valid.set, type = "class")
dim(Prediction)
length(Prediction)
dim(valid.set)
Prediction
sum(valid.set$PC == Prediction)
length(Prediction)
16998/24252
fit <- rpart(PC ~ married_couple + C_previous + C + group_size, data=train.set, method="class")
Prediction <- predict(fit, valid.set, type = "class")
sum(valid.set$PC == Prediction)
fit <- rpart(PC ~ married_couple + C_previous + C + risk_factor, data=train.set, method="class")
Prediction <- predict(fit, valid.set, type = "class")
sum(valid.set$PC == Prediction)
fit <- rpart(PC ~ married_couple + C_previous + C, data=train.set, method="class")
Prediction <- predict(fit, valid.set, type = "class")
length(Prediction)
sum(valid.set$PC == Prediction)
hist(quotesTop$age_oldest)
?hist
hist(quotesTop$age_oldest)
hist(quotesTop$age_oldest, breaks=2)
hist(quotesTop$age_oldest, breaks=40)
length(quotesTop[quotesTop$age_oldest > 74]$age_oldest)
length(quotesTop[quotesTop$age_oldest > 74,]$age_oldest)
fit <- rpart(PC ~ C_previous + C, data=train.set, method="class")
Prediction <- predict(fit, valid.set, type = "class")
length(Prediction)
sum(valid.set$PC == Prediction)
require(nnet)
model_off <- train(PC ~ C_previous + C, train.set, method='nnet', linout=TRUE, trace = FALSE);
fit <- rpart(PC ~ C_previous + C, data=train.set, method="class")
quotesTop$PredictionC <- predict(fit, valid.set, type = "class")
fit <- rpart(PC ~ C_previous + C, data=quotesTop, method="class")
quotesTop$PredictionC <- predict(fit, valid.set, type = "class")
quotesTop$PredictionC <- predict(fit, quotesTop, type = "class")
fitA <- rpart(PA ~ married_couple + homeowner + A, data=quotesTop, method="class")
Prediction <- predict(fitC, quotesTop, type = "class")
Prediction <- predict(fitA, quotesTop, type = "class")
sum(valid.set$PA == Prediction)
sum(quotesTop$PA == Prediction)
fitA <- rpart(PA ~ married_couple + homeowner + A + PredictionC + C, data=quotesTop, method="class")
quotesTop$PredictionA <- predict(fitA, quotesTop, type = "class")
sum(quotesTop$PA == quotesTop$PredictionA)
fitB <- rpart(PB ~ married_couple + homeowner + B + A + PredictionA + PredictionC + C, data=quotesTop, method="class")
quotesTop$PredictionB <- predict(fitB, quotesTop, type = "class")
sum(quotesTop$PA == quotesTop$PredictionA)
sum(quotesTop$PB == quotesTop$PredictionB)
fitD <- rpart(PD ~ married_couple + homeowner + B + A + PredictionA + PredictionC + C + PredictionB + B, data=quotesTop, method="class")
quotesTop$PredictionD <- predict(fitD, quotesTop, type = "class")
sum(quotesTop$PD == quotesTop$PredictionD)
fitE <- rpart(PE ~ married_couple + homeowner + B + A + PredictionA + PredictionC + C + PredictionB + B + PredictionD + D, data=quotesTop, method="class")
quotesTop$PredictionE <- predict(fitE, quotesTop, type = "class")
sum(quotesTop$PD == quotesTop$PredictionD)
sum(quotesTop$PE == quotesTop$PredictionE)
fitF <- rpart(PF ~ married_couple + homeowner + B + A + PredictionA + PredictionC + C + PredictionB + B + PredictionD + D + PredictionE + E, data=quotesTop, method="class")
quotesTop$PredictionF <- predict(fitF, quotesTop, type = "class")
fitG <- rpart(PG ~ married_couple + homeowner + A + PredictionA + PredictionC + C + PredictionB + B + PredictionD + D + PredictionE + E + PredictionF + F, data=quotesTop, method="class")
quotesTop$PredictionG <- predict(fitG, quotesTop, type = "class")
purchases.test <- mytest[ !duplicated( mytest$customer_ID, fromLast=TRUE ) , ]
#first record
quotesTop.test <- mytest[ mytest$record_type == 0, ]
quotesTop.test <- quotesTop.test[ !duplicated( quotesTop.test$customer_ID, fromLast=FALSE ), ]
# remove NAs
quotesTop.test$C_previous[is.na( quotesTop.test$C_previous )] <- 0
purchases.test$C_previous[is.na( purchases.test$C_previous )] <- 0
#Add the last record value for the A-G Items
quotesTop.test$PA <- purchase.test$A
quotesTop.test$PB <- purchase.test$B
quotesTop.test$PC <- purchase.test$C
quotesTop.test$C_previous[is.na( quotesTop.test$C_previous )] <- 0
purchases.test$C_previous[is.na( purchases.test$C_previous )] <- 0
#Add the last record value for the A-G Items
quotesTop.test$PA <- purchases.test$A
quotesTop.test$PB <- purchases.test$B
quotesTop.test$PC <- purchases.test$C
quotesTop.test$PD <- purchases.test$D
quotesTop.test$PE <- purchases.test$E
quotesTop.test$PF <- purchases.test$F
quotesTop.test$PG <- purchases.test$G
quotesTop.test$PredictionC <- predict(fitC, quotesTop.test, type = "class")
fitC <- rpart(PC ~ C_previous + C, data=quotesTop, method="class")
quotesTop.test$PredictionC <- predict(fitC, quotesTop.test, type = "class")
quotesTop.test$PredictionA <- predict(fitA, quotesTop.test, type = "class")
quotesTop.test$PredictionB <- predict(fitB, quotesTop.test, type = "class")
quotesTop.test$PredictionD <- predict(fitD, quotesTop.test, type = "class")
quotesTop.test$PredictionE <- predict(fitE, quotesTop.test, type = "class")
quotesTop.test$PredictionF <- predict(fitF, quotesTop.test, type = "class")
quotesTop.test$PredictionG <- predict(fitG, quotesTop.test, type = "class")
SubValues <- paste0(quotesTop.test$PredictionA, quotesTop.test$PredictionB, quotesTop.test$PredictionC, quotesTop.test$PredictionD, quotesTop.test$PredictionE, quotesTop.test$PredictionF, quotesTop.test$PredictionG)
SubValues
str(quotesTop.test)
subM <- cbind(quotesTop.test$customer_ID,SubValues)
subM
write.csv(subM,paste0('sub.persistent.csv'),quote=FALSE , row.names = FALSE )
write.csv(subM,paste0('sub.persistent.csv'),quote=FALSE , row.names = FALSE )
write.csv(subM,paste0('sub.persistent.csv'),quote=FALSE , row.names = FALSE )
subM <- cbind(quotesTop.test$customer_ID,SubValues)
colnames(subM) <- c("customer_ID","plan")
write.csv(subM,paste0('sub.persistent.csv'),quote=FALSE , row.names = FALSE )
fitC <- rpart(PC ~ C_previous + C, data=quotesTop, method="class")
quotesTop$PredictionC <- predict(fitC, quotesTop, type = "class")
fitA <- rpart(PA ~ married_couple + homeowner + A + C, data=quotesTop, method="class")
quotesTop$PredictionA <- predict(fitA, quotesTop, type = "class")
fitB <- rpart(PB ~ married_couple + homeowner + B + PredictionC + C, data=quotesTop, method="class")
quotesTop$PredictionB <- predict(fitB, quotesTop, type = "class")
fitD <- rpart(PD ~ married_couple + homeowner + D + PredictionC + C, data=quotesTop, method="class")
quotesTop$PredictionD <- predict(fitD, quotesTop, type = "class")
fitE <- rpart(PE ~ married_couple + homeowner + E + PredictionC + C, data=quotesTop, method="class")
quotesTop$PredictionE <- predict(fitE, quotesTop, type = "class")
fitF <- rpart(PF ~ married_couple + homeowner + F + PredictionC + C, data=quotesTop, method="class")
quotesTop$PredictionF <- predict(fitF, quotesTop, type = "class")
fitG <- rpart(PG ~ married_couple + homeowner + G + PredictionC + C, data=quotesTop, method="class")
quotesTop$PredictionG <- predict(fitG, quotesTop, type = "class")
predValues <- paste0(quotesTop$PredictionA, quotesTop$PredictionB, quotesTop$PredictionC, quotesTop$PredictionD, quotesTop$PredictionE, quotesTop$PredictionF, quotesTop$PredictionG)
require('klaR')
?errormatrix
errormatrix(predValues, actualValues, relative = F)
actualValues <- paste0(quotesTop$PredictionPA, quotesTop$PredictionPB, quotesTop$PredictionPC, quotesTop$PredictionPD, quotesTop$PredictionPE, quotesTop$PredictionPF, quotesTop$PredictionPG)
errormatrix(predValues, actualValues, relative = F)
predValues <- paste0(quotesTop$PredictionA, quotesTop$PredictionB, quotesTop$PredictionC, quotesTop$PredictionD, quotesTop$PredictionE, quotesTop$PredictionF, quotesTop$PredictionG)
actualValues <- paste0(quotesTop$PredictionPA, quotesTop$PredictionPB, quotesTop$PredictionPC, quotesTop$PredictionPD, quotesTop$PredictionPE, quotesTop$PredictionPF, quotesTop$PredictionPG)
require('klaR')
errormatrix(predValues, actualValues, relative = F)
length(predValues)
length(actualValues)
actualValues <- paste0(quotesTop$PA, quotesTop$PB, quotesTop$PC, quotesTop$PD, quotesTop$PE, quotesTop$PF, quotesTop$PG)
length(actualValues)
errormatrix(predValues, actualValues, relative = F)
predValues
actualValues
errormatrix(predValues, actualValues, relative = F)
sum(predValues==actualValues)/length(actualValues)
install.packages('randomForest')
library(randomForest)
fitC <- randomForest(PC ~ C_previous + C, data=quotesTop, importance=TRUE, ntree=2000)
View(dd)
save.image("~/.RData")
install.packages("NMMAPSdata", contriburl = "http://www.ihapss.jhsph.edu/data/NMMAPS/R/", type = "source")
install.packages("NMMAPSdata", contriburl = "http://www.ihapss.jhsph.edu/data/NMMAPS/R/windows")
library('NMMAPSdata')
update.packages(checkBuilt = TRUE, ask = FALSE)
library('NMMAPSdata')
load("C:/Program Files/R/R-3.1.0/library/NMMAPSdata/db/city-data/chic.rda.bz2")
str(chic)
nmmaps<-chic
rm(chic)
nmmaps$date<-as.Date(nmmaps$date)
nmmaps$date<-as.Date(nmmaps$date, origin="1996-12-31")
nmmaps$date
?as.Date
load("C:/Program Files/R/R-3.1.0/library/NMMAPSdata/db/city-data/chic.rda.bz2")
nmmaps<-chic
nmmaps$date<-as.Date(nmmaps$date, "%Y%b%d")
head(nmmaps$date)
nmmaps$date<-as.Date(nmmaps$date, "%Y%M%D")
nmmaps$date<-as.Date(nmmaps$date, "%Y%M%d")
class(nmmaps$date)
nmmaps$date<-as.Date(nmmaps$date, "%Y %M %d")
nmmaps$date<-as.Date(nmmaps$date, "%Y%m%d")
head(nmmaps$date)
tail(nmmaps$date, 100)
head(nmmaps)
temp <- "Sdfsd"
class(temp)
nmmaps$date<-as.Date(as.character(nmmaps$date), "%Y%m%d")
head(nmmaps$date)
nmmaps$year<-substring(nmmaps$date,1,4)
head(nmmaps$year)
g<-ggplot(nmmaps, aes(date, temp))+geom_point(color="firebrick")
library(ggplot2)
g<-ggplot(nmmaps, aes(date, temp))+geom_point(color="firebrick")
g
nmmaps <- nmmaps[year>1996 & year<2001,]
nmmaps <- nmmaps[nmmaps$year>1996 & nmmaps$year<2001,]
g<-ggplot(nmmaps, aes(date, temp))+geom_point(color="firebrick")
g
head(nmmaps$temp)
rm(temp)
g<-ggplot(nmmaps, aes(date, temp))+geom_point(color="firebrick")
g
g<-ggplot(nmmaps, aes(date, tmpd))+geom_point(color="firebrick")
g
g<-g+ggtitle('Temperature')
g
g+theme(plot.title = element_text(size=20, face="bold", vjust=2))
g
g<-g+theme(plot.title = element_text(size=20, face="bold", vjust=2))
g
library(extrafont)
install.packages('extrafont ')
install.packages('extrafont')
library(extrafont)
g+theme(plot.title = element_text(size=30,lineheight=.8,
vjust=1,family="Bauhaus 93"))
library(extrafont)
g+theme(plot.title = element_text(size=30,lineheight=.8,
vjust=1,family="Elephant"))
library(extrafont)
g+theme(plot.title = element_text(size=30,lineheight=.8,
vjust=1,family="Courier New"))
g+theme(plot.title = element_text(size=30,lineheight=.8,
vjust=1,family="Courier New"))
g<-g+ggtitle("This is a longer\ntitle than expected")
g
g+theme(plot.title = element_text(size=20, face="bold", vjust=1, lineheight=0.6))
g<-g+ggtitle("This is a longer\ntitle than expected")
g
g+theme(plot.title = element_text(size=20, face="bold", vjust=1, lineheight=0.6))
g+theme(plot.title = element_text(size=20, face="bold", vjust=1, lineheight=10.6))
g
g+theme(plot.title = element_text(size=20, face="bold", vjust=1, lineheight=10.6))
g+theme(plot.title = element_text(size=20, face="bold", vjust=1, lineheight=0.6))
g<-g+labs(x="Date", y=expression(paste("Temperature ( ", degree ~ F, " )")), title="Temperature")
g
season
install.packages("dplyr")
vignette("introduction", package = "dplyr")
library(hflights)
dim(hflights)
head(hflights)
hflights_df <- tbl_df(hflights)
library(dplyr)
hflights_df <- tbl_df(hflights)
hflights_df
filter(hflights_df, Month == 1, DayofMonth == 1)
filter(hflights_df, Month == 1 | Month == 2)
filter(hflights_df, Month == 1 , DayofMonth == 1 | DayofMonth == 2)
x <- data.frame()
x$EventID <- c(1:6)
x[,1] <- c(1:6)
x[1:6,1] <- c(1:6)
x[1:6,2] <- c(1:6)
x[1:6,3] <- c(1:6)
colnames(x) <- c("EventId", "Label", "N")
str(x)
temp <- x[,-c("EventId", "Label")];
temp <- x[,c("EventId", "Label")];
temp <- x[,-c];
c1 <- c("EventId", "Label")
temp <- x[,-c1];
temp <- x[,!c1];
temp <- x[,!c("EventId", "Label")];
select(x, !c1)
library(dplyr)
select(x, !c1)
select(x, c1)
?select
select(x, -c1)
c1 <- c("EventId", "Label")
select(x, -EventId,-Label)
install.packages('AppliedPredictiveModeling')
library(AppliedPredictiveModeling)
data(segmentationOriginal)
segData <- subset(segmentationOriginal, Case == "Train")
head(segmentationOriginal)
head(segmentationOriginal[,Case])
head(segmentationOriginal[,'Case'])
library(dplyr)
?select
segData <- filter(segmentationOriginal, Case == "Train")
segData <- select(segData, -(1:3))
segData <- filter(segmentationOriginal, Case == "Train")
cellID <- segData$Cell
class <- segData$Class
case <- segData$Case
segData <- select(segData, -(1:3))
?grep
statusColNum <- grep("Status", names(segData))
length(statusColNum)
head(statusColNum)
head(names(segData))
segData <- select(segData, -statusColNum)
head(names(segData))
library(e1071)
skewness(segData$AngleCh1)
hist(segData$AngleCh1)
histogram(segData$AngleCh1)
library(caret)
histogram(segData$AngleCh1)
Ch1AreaTrans <- BoxCoxTrans(segData$AreaCh1)
Ch1AreaTrans
?prcomp
histogram(Ch1AreaTrans)
head(Ch1AreaTrans)
predict(Ch1AreaTrans, head(segData$AreaCh1))
histogram(predict(Ch1AreaTrans, segData$AreaCh1))
skewness(predict(Ch1AreaTrans, head(segData$AreaCh1)))
skewness(segData$AngleCh1)
plot(segData$AngleCh1)
plot(predict(Ch1AreaTrans, segData$AreaCh1))
histogram(predict(Ch1AreaTrans, segData$AreaCh1))
pcaObject <- prcomp(segData, center = TRUE, scale. = TRUE)
pcaObject
str(pcaObject)
head(pcaObject$x[, 1:5])
head(pcaObject$rotation[, 1:3])
nearZeroVar(segData)
library(corrplot)
install.packages('corrplot')
library(corrplot)
correlations <- cor(segData)
dim(correlations)
correlations[1:4, 1:4]
correlations[1:10, 1:10]
correlations[1:8, 1:8]
correlations[1:7, 1:7]
corrplot(correlations, order = "hclust")
highCorr <- findCorrelation(correlations, cutoff = .75)
length(highCorr)
highCorr
head(highCorr)
filteredSegData <- segData[, -highCorr]
dim(filteredSegData)
?splom
??splom
x <- c(1:6)
m <- matrix(0, 6, 6)
m
for(i in 1:length(x)){
m[i, x[i]] = 1
}
m
setwd("C:/Users/GALT/Desktop/Forest Cover Type Prediction/code/submissions/0.78553")
library(plyr)
library(dplyr)
library(caret)
library(extraTrees)
setwd("C:/Users/GALT/Desktop/Forest Cover Type Prediction/code/submissions/New folder")
####################################
# Train Set
####################################
train.set = read.csv(file='../../../data/train.csv',head=TRUE,sep=",",stringsAsFactors=F)
train.set$Soil_Type7 = NULL
train.set$Soil_Type15 = NULL
X.train <- select(train.set, Elevation:Horizontal_Distance_To_Fire_Points)
#hydrology lower or higher
X.train$HydrologyVerticalOrientation <- (X.train$Vertical_Distance_To_Hydrology > 0) + 0
X.train$Horizontal_Hydrology_Roadways <- abs(X.train$Horizontal_Distance_To_Roadways) - abs(X.train$Horizontal_Distance_To_Hydrology)
X.train$Horizontal__Hydrology_Fire_Points <- abs(X.train$Horizontal_Distance_To_Hydrology) - abs(X.train$Horizontal_Distance_To_Fire_Points)
X.train$Elevation <- log(X.train$Elevation)
#Total Hillshade
X.train$Hillshade_Total <- with(X.train, Hillshade_9am + Hillshade_Noon + Hillshade_3pm)
X.train <- cbind(X.train, select(train.set, Wilderness_Area1:Soil_Type40))
y.train <- train.set[,c("Id","Cover_Type")]
####################################
# Test Set
####################################
test.set = read.csv(file='../../../data/test.csv',head=TRUE,sep=",",stringsAsFactors=F)
test.set$Soil_Type7 = NULL
test.set$Soil_Type15 = NULL
X.test <- select(test.set, Elevation:Horizontal_Distance_To_Fire_Points)
#hydrology lower or higher
X.test$HydrologyVerticalOrientation <- (X.test$Vertical_Distance_To_Hydrology > 0) + 0
X.test$Horizontal_Hydrology_Roadways <- abs(X.test$Horizontal_Distance_To_Roadways) - abs(X.test$Horizontal_Distance_To_Hydrology)
X.test$Horizontal__Hydrology_Fire_Points <- abs(X.test$Horizontal_Distance_To_Hydrology) - abs(X.test$Horizontal_Distance_To_Fire_Points)
X.test$Elevation <- log(X.test$Elevation)
#Total Hillshade
X.test$Hillshade_Total <- with(X.test, Hillshade_9am + Hillshade_Noon + Hillshade_3pm)
X.test <- cbind(X.test, select(test.set, Wilderness_Area1:Soil_Type40))
####################################
# Scale and Center
####################################
X.test$type <- "test"
X.train$type <- "train"
X.all <- rbind(X.test, X.train)
temp <- X.all[,"type"]
X.all[,"type"] <- NULL
X.all <- data.frame(Id=c(test.set$Id, train.set$Id), scale(X.all, center=T, scale=T))
X.all$type <- temp
X.train <- X.all[X.all$type=="train",]
X.train$type <- NULL
X.test <- X.all[X.all$type=="test",]
X.test$type <- NULL
rm(X.all, temp)
Y_eT <- data.frame(Id=test.set[,"Id"], Cover_Type=NA)
X.train$Id <- NULL
X.test$Id <- NULL
####################################
# Fit Model
####################################
eT_fit <- extraTrees(X.train
,as.factor(y.train[,2])
,ntree=500
,nodesize = 1
,numRandomCuts = 5
,evenCuts = FALSE
,numThreads = 6
)
Y_eT[, 2] <- predict(eT_fit, X.test)
example = read.csv(file='../../../data/sampleSubmission.csv',head=TRUE,sep=",")
example$Id <- Y_eT[, 1]
example$Cover_Type <- as.numeric(Y_eT[, 2])
write.csv((example),file='submission.csv',row.names=FALSE)
setwd("C:/Users/GALT/Desktop/Forest Cover Type Prediction - Copy/code")
library(plyr)
library(dplyr)
library(caret)
library(extraTrees)
train.set = read.csv(file='../data/train.csv',head=TRUE,sep=",",stringsAsFactors=F)
train.set$Soil_Type7 = NULL
train.set$Soil_Type15 = NULL
X <- select(train.set, Elevation:Horizontal_Distance_To_Fire_Points)
####################################
# Feature Eng.
####################################
#hydrology lower or higher
X$HydrologyVerticalOrientation <- (X$Vertical_Distance_To_Hydrology > 0) + 0
X$Horizontal_Hydrology_Roadways <- abs(X$Horizontal_Distance_To_Roadways) - abs(X$Horizontal_Distance_To_Hydrology)
X$Horizontal__Hydrology_Fire_Points <- abs(X$Horizontal_Distance_To_Hydrology) - abs(X$Horizontal_Distance_To_Fire_Points)
X$Horizontal__Hydrology_Fire_Points[is.infinite(X$Horizontal__Hydrology_Fire_Points)] <- 0
X$Elevation <- log(X$Elevation)
#Total Hillshade
X$Hillshade_Total <- with(X, Hillshade_9am + Hillshade_Noon + Hillshade_3pm)
X <- cbind(X, select(train.set, Wilderness_Area1:Soil_Type40))
X <- data.frame(scale(X, center=T, scale=T))
y <- train.set[,c("Id","Cover_Type")]
#results matrix
Y_eT <- y
Y_eT[,-1] <- NA
####################################
# Create folds
####################################
set.seed(117)
folds <- createFolds(y$Cover_Type, k=3, list=T) #10 fold cross validation
####################################
# Training parameters
####################################
foldIndex <- 0;
for(i in 1:length(folds)){
print(paste0("i: ", i))
fold.X.train <- X[-folds[[i]],]
fold.X.test <- X[folds[[i]],]
fold.y.train <- y[-folds[[i]],]
fold.y.test <- y[folds[[i]],]
trgtRows <- c((foldIndex+1):(foldIndex+length(folds[[i]])))
##ET
Y_eT[trgtRows, "Id"] <- fold.y.test[,"Id"]
eT_fit <- extraTrees(fold.X.train
,as.factor(fold.y.train[,2])
,ntree=500
,nodesize = 1
,numRandomCuts = 5
,evenCuts = FALSE
,numThreads = 6
)
Y_eT[trgtRows, 2] <- predict(eT_fit, fold.X.test)
foldIndex <- foldIndex + length(folds[[i]])
}
Y_eT <- arrange(Y_eT, Id)
confusionMatrix(Y_eT[,2], y[,2])
sum(y[,2] == Y_eT[,2])/length(y[,2])
